{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rcnn-text-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyObuWuH6LssQWsE9phb6RrR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramprabu95/AI_based_image_compression/blob/main/rcnn_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5ECmMfRpDQJ0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device('cuda')\n",
        "class RCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Recurrent Convolutional Neural Networks for Text Classification (2015)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, hidden_size_linear, class_num, dropout):\n",
        "        super(RCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.W = nn.Linear(embedding_dim + 2*hidden_size, hidden_size_linear)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc = nn.Linear(hidden_size_linear, class_num)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = |bs, seq_len|\n",
        "        x_emb = self.embedding(x)\n",
        "        # x_emb = |bs, seq_len, embedding_dim|\n",
        "        output, _ = self.lstm(x_emb)\n",
        "        # output = |bs, seq_len, 2*hidden_size|\n",
        "        output = torch.cat([output, x_emb], 2)\n",
        "        # output = |bs, seq_len, embedding_dim + 2*hidden_size|\n",
        "        output = self.tanh(self.W(output)).transpose(1, 2)\n",
        "        # output = |bs, seq_len, hidden_size_linear| -> |bs, hidden_size_linear, seq_len|\n",
        "        output = F.max_pool1d(output, output.size(2)).squeeze(2)\n",
        "        # output = |bs, hidden_size_linear|\n",
        "        output = self.fc(output)\n",
        "        # output = |bs, class_num|\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, dictionary):\n",
        "        # Unknown Token is index 1 (<UNK>)\n",
        "        self.x = [[dictionary.get(token, 1) for token in token_list] for token_list in texts]\n",
        "        self.y = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the data length\"\"\"\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return one item on the index\"\"\"\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "\n",
        "def collate_fn(data, max_len, pad_idx=0):\n",
        "    \"\"\"Padding\"\"\"\n",
        "    texts, labels = zip(*data)\n",
        "    texts = [s + [pad_idx] * (max_len - len(s)) if len(s) < max_len else s[:max_len] for s in texts]\n",
        "    return torch.LongTensor(texts), torch.LongTensor(labels)"
      ],
      "metadata": {
        "id": "i0L4UQyAGSCq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def read_file(file_path):\n",
        "    \"\"\"\n",
        "    Read function for AG NEWS Dataset\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(file_path, names=[\"class\", \"title\", \"description\"])\n",
        "    texts = list(data['title'].values + ' ' + data['description'].values)\n",
        "    texts = [word_tokenize(preprocess_text(sentence)) for sentence in texts]\n",
        "    labels = [label-1 for label in list(data['class'].values)]  # label : 1~4  -> label : 0~3\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "def preprocess_text(string):\n",
        "    \"\"\"\n",
        "    reference : https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = string.lower()\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip()\n",
        "\n",
        "\n",
        "def metrics(dataloader, losses, correct, y_hats, targets):\n",
        "    avg_loss = losses / len(dataloader)\n",
        "    accuracy = correct / len(dataloader.dataset) * 100\n",
        "    precision = precision_score(targets, y_hats, average='macro')\n",
        "    recall = recall_score(targets, y_hats, average='macro')\n",
        "    f1 = f1_score(targets, y_hats, average='macro')\n",
        "    cm = confusion_matrix(targets, y_hats)\n",
        "    return avg_loss, accuracy, precision, recall, f1, cm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye7NXUTeGa8g",
        "outputId": "462a753f-7d8d-4628-cefa-f86fb6950b54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "def train(model, optimizer1,optimizer2, train_dataloader, valid_dataloader, epochs):\n",
        "    best_f1 = 0\n",
        "    print('Start Training!')\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        for step, (x, y) in enumerate(train_dataloader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss = F.cross_entropy(pred, y)\n",
        "            if loss>0.3:\n",
        "              optimizer1.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer1.step()\n",
        "            else:\n",
        "              optimizer2.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer2.step()\n",
        "\n",
        "            if (step+1) % 200 == 0:\n",
        "                print(f'|EPOCHS| {epoch:>}/{epochs} |STEP| {step+1:>4}/{len(train_dataloader)} |LOSS| {loss.item():>.4f}')\n",
        "\n",
        "        avg_loss, accuracy, _, _, f1, _ = evaluate(model, valid_dataloader)\n",
        "        print('-'*50)\n",
        "        print(f'|* VALID SET *| |VAL LOSS| {avg_loss:>.4f} |ACC| {accuracy:>.4f} |F1| {f1:>.4f}')\n",
        "        print('-'*50)\n",
        "        PATH = './best.pth'\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            print(f'Saving best model... F1 score is {best_f1:>.4f}')\n",
        "            torch.save(model.state_dict(),PATH)\n",
        "            print('Model saved!')\n",
        "\n",
        "\n",
        "def evaluate(model, valid_dataloader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        losses, correct = 0, 0\n",
        "        y_hats, targets = [], []\n",
        "        for x, y in valid_dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            pred = model(x)\n",
        "            loss = F.cross_entropy(pred, y)\n",
        "            losses += loss.item()\n",
        "\n",
        "            y_hat = torch.max(pred, 1)[1]\n",
        "            y_hats += y_hat.tolist()\n",
        "            targets += y.tolist()\n",
        "            correct += (y_hat == y).sum().item()\n",
        "\n",
        "    avg_loss, accuracy, precision, recall, f1, cm = metrics(valid_dataloader, losses, correct, y_hats, targets)\n",
        "    return avg_loss, accuracy, precision, recall, f1, cm\n",
        "\n"
      ],
      "metadata": {
        "id": "y3AphBjjGxeQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_dictionary(texts, vocab_size):\n",
        "    counter = Counter()\n",
        "    SPECIAL_TOKENS = ['<PAD>', '<UNK>']\n",
        "\n",
        "    for word in texts:\n",
        "        counter.update(word)\n",
        "\n",
        "    words = [word for word, count in counter.most_common(vocab_size - len(SPECIAL_TOKENS))]\n",
        "    words = SPECIAL_TOKENS + words\n",
        "    word2idx = {word: idx for idx, word in enumerate(words)}\n",
        "\n",
        "    return word2idx"
      ],
      "metadata": {
        "id": "lN1vjZz2K3iC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3gc-QgoSRDHt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "\n",
        "# Mounting google drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOt4U1hCRvGi",
        "outputId": "4c68397d-e4a9-4385-b4d8-239c62629556"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = '/content/drive/MyDrive/Colab Notebooks/data_text/train.csv'\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "model = RCNN(vocab_size=8000,\n",
        "            embedding_dim=300,\n",
        "            hidden_size=512,\n",
        "            hidden_size_linear=512,\n",
        "            class_num=4,\n",
        "            dropout=0.0).to(device)\n",
        "    \n",
        "train_texts, train_labels = read_file(train_file_path)\n",
        "word2idx = build_dictionary(train_texts, vocab_size=8000)\n",
        "full_dataset = CustomTextDataset(train_texts, train_labels, word2idx)\n",
        "num_train_data = len(full_dataset) - 10000\n",
        "train_dataset, val_dataset = random_split(full_dataset, [num_train_data, 10000])\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                                  collate_fn=lambda x: collate_fn(x, 64),\n",
        "                                  batch_size=64,\n",
        "                                  shuffle=True)\n",
        "\n",
        "valid_dataloader = DataLoader(dataset=val_dataset,\n",
        "                                  collate_fn=lambda x: collate_fn(x, 64),\n",
        "                                  batch_size=64,\n",
        "                                  shuffle=True)\n",
        "\n",
        "optimizer1 = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "optimizer2 = torch.optim.Adam(model.parameters(), lr=3e-6)\n",
        "train(model, optimizer1,optimizer2, train_dataloader, valid_dataloader,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdYhPSlhLCZp",
        "outputId": "d9b9d9e5-75fd-4849-8e3d-30ee185f1101"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Training!\n",
            "|EPOCHS| 1/10 |STEP|  200/1719 |LOSS| 0.4706\n",
            "|EPOCHS| 1/10 |STEP|  400/1719 |LOSS| 0.3854\n",
            "|EPOCHS| 1/10 |STEP|  600/1719 |LOSS| 0.5302\n",
            "|EPOCHS| 1/10 |STEP|  800/1719 |LOSS| 0.3202\n",
            "|EPOCHS| 1/10 |STEP| 1000/1719 |LOSS| 0.4386\n",
            "|EPOCHS| 1/10 |STEP| 1200/1719 |LOSS| 0.4680\n",
            "|EPOCHS| 1/10 |STEP| 1400/1719 |LOSS| 0.5420\n",
            "|EPOCHS| 1/10 |STEP| 1600/1719 |LOSS| 0.2532\n",
            "--------------------------------------------------\n",
            "|* VALID SET *| |VAL LOSS| 0.3024 |ACC| 89.7800 |F1| 0.8978\n",
            "--------------------------------------------------\n",
            "Saving best model... F1 score is 0.8978\n",
            "Model saved!\n",
            "|EPOCHS| 2/10 |STEP|  200/1719 |LOSS| 0.2739\n",
            "|EPOCHS| 2/10 |STEP|  400/1719 |LOSS| 0.3859\n",
            "|EPOCHS| 2/10 |STEP|  600/1719 |LOSS| 0.1683\n",
            "|EPOCHS| 2/10 |STEP|  800/1719 |LOSS| 0.3818\n",
            "|EPOCHS| 2/10 |STEP| 1000/1719 |LOSS| 0.2155\n",
            "|EPOCHS| 2/10 |STEP| 1200/1719 |LOSS| 0.1619\n",
            "|EPOCHS| 2/10 |STEP| 1400/1719 |LOSS| 0.2700\n",
            "|EPOCHS| 2/10 |STEP| 1600/1719 |LOSS| 0.2503\n",
            "--------------------------------------------------\n",
            "|* VALID SET *| |VAL LOSS| 0.2726 |ACC| 90.6800 |F1| 0.9064\n",
            "--------------------------------------------------\n",
            "Saving best model... F1 score is 0.9064\n",
            "Model saved!\n",
            "|EPOCHS| 3/10 |STEP|  200/1719 |LOSS| 0.2775\n",
            "|EPOCHS| 3/10 |STEP|  400/1719 |LOSS| 0.3172\n",
            "|EPOCHS| 3/10 |STEP|  600/1719 |LOSS| 0.2159\n",
            "|EPOCHS| 3/10 |STEP|  800/1719 |LOSS| 0.1628\n",
            "|EPOCHS| 3/10 |STEP| 1000/1719 |LOSS| 0.3072\n",
            "|EPOCHS| 3/10 |STEP| 1200/1719 |LOSS| 0.1625\n",
            "|EPOCHS| 3/10 |STEP| 1400/1719 |LOSS| 0.2180\n",
            "|EPOCHS| 3/10 |STEP| 1600/1719 |LOSS| 0.2527\n",
            "--------------------------------------------------\n",
            "|* VALID SET *| |VAL LOSS| 0.2697 |ACC| 90.8800 |F1| 0.9083\n",
            "--------------------------------------------------\n",
            "Saving best model... F1 score is 0.9083\n",
            "Model saved!\n",
            "|EPOCHS| 4/10 |STEP|  200/1719 |LOSS| 0.1697\n",
            "|EPOCHS| 4/10 |STEP|  400/1719 |LOSS| 0.1728\n",
            "|EPOCHS| 4/10 |STEP|  600/1719 |LOSS| 0.2225\n",
            "|EPOCHS| 4/10 |STEP|  800/1719 |LOSS| 0.2446\n",
            "|EPOCHS| 4/10 |STEP| 1000/1719 |LOSS| 0.1655\n",
            "|EPOCHS| 4/10 |STEP| 1200/1719 |LOSS| 0.2566\n",
            "|EPOCHS| 4/10 |STEP| 1400/1719 |LOSS| 0.2180\n",
            "|EPOCHS| 4/10 |STEP| 1600/1719 |LOSS| 0.2445\n",
            "--------------------------------------------------\n",
            "|* VALID SET *| |VAL LOSS| 0.2610 |ACC| 91.1900 |F1| 0.9117\n",
            "--------------------------------------------------\n",
            "Saving best model... F1 score is 0.9117\n",
            "Model saved!\n",
            "|EPOCHS| 5/10 |STEP|  200/1719 |LOSS| 0.2336\n",
            "|EPOCHS| 5/10 |STEP|  400/1719 |LOSS| 0.1444\n",
            "|EPOCHS| 5/10 |STEP|  600/1719 |LOSS| 0.2720\n",
            "|EPOCHS| 5/10 |STEP|  800/1719 |LOSS| 0.3026\n",
            "|EPOCHS| 5/10 |STEP| 1000/1719 |LOSS| 0.2545\n",
            "|EPOCHS| 5/10 |STEP| 1200/1719 |LOSS| 0.2003\n",
            "|EPOCHS| 5/10 |STEP| 1400/1719 |LOSS| 0.2395\n",
            "|EPOCHS| 5/10 |STEP| 1600/1719 |LOSS| 0.2582\n",
            "--------------------------------------------------\n",
            "|* VALID SET *| |VAL LOSS| 0.2610 |ACC| 91.1500 |F1| 0.9114\n",
            "--------------------------------------------------\n",
            "|EPOCHS| 6/10 |STEP|  200/1719 |LOSS| 0.2495\n",
            "|EPOCHS| 6/10 |STEP|  400/1719 |LOSS| 0.1649\n",
            "|EPOCHS| 6/10 |STEP|  600/1719 |LOSS| 0.2300\n",
            "|EPOCHS| 6/10 |STEP|  800/1719 |LOSS| 0.1565\n",
            "|EPOCHS| 6/10 |STEP| 1000/1719 |LOSS| 0.1741\n",
            "|EPOCHS| 6/10 |STEP| 1200/1719 |LOSS| 0.1789\n",
            "|EPOCHS| 6/10 |STEP| 1400/1719 |LOSS| 0.1218\n",
            "|EPOCHS| 6/10 |STEP| 1600/1719 |LOSS| 0.1808\n",
            "--------------------------------------------------\n",
            "|* VALID SET *| |VAL LOSS| 0.2561 |ACC| 91.3300 |F1| 0.9131\n",
            "--------------------------------------------------\n",
            "Saving best model... F1 score is 0.9131\n",
            "Model saved!\n",
            "|EPOCHS| 7/10 |STEP|  200/1719 |LOSS| 0.1191\n",
            "|EPOCHS| 7/10 |STEP|  400/1719 |LOSS| 0.1562\n",
            "|EPOCHS| 7/10 |STEP|  600/1719 |LOSS| 0.2644\n",
            "|EPOCHS| 7/10 |STEP|  800/1719 |LOSS| 0.1990\n",
            "|EPOCHS| 7/10 |STEP| 1000/1719 |LOSS| 0.1057\n",
            "|EPOCHS| 7/10 |STEP| 1200/1719 |LOSS| 0.3433\n",
            "|EPOCHS| 7/10 |STEP| 1400/1719 |LOSS| 0.1283\n",
            "|EPOCHS| 7/10 |STEP| 1600/1719 |LOSS| 0.1036\n",
            "--------------------------------------------------\n",
            "|* VALID SET *| |VAL LOSS| 0.2531 |ACC| 91.3400 |F1| 0.9131\n",
            "--------------------------------------------------\n",
            "|EPOCHS| 8/10 |STEP|  200/1719 |LOSS| 0.1672\n",
            "|EPOCHS| 8/10 |STEP|  400/1719 |LOSS| 0.0896\n",
            "|EPOCHS| 8/10 |STEP|  600/1719 |LOSS| 0.1387\n",
            "|EPOCHS| 8/10 |STEP|  800/1719 |LOSS| 0.1901\n",
            "|EPOCHS| 8/10 |STEP| 1000/1719 |LOSS| 0.1824\n",
            "|EPOCHS| 8/10 |STEP| 1200/1719 |LOSS| 0.1761\n",
            "|EPOCHS| 8/10 |STEP| 1400/1719 |LOSS| 0.1543\n",
            "|EPOCHS| 8/10 |STEP| 1600/1719 |LOSS| 0.1946\n",
            "--------------------------------------------------\n",
            "|* VALID SET *| |VAL LOSS| 0.2596 |ACC| 91.1900 |F1| 0.9117\n",
            "--------------------------------------------------\n",
            "|EPOCHS| 9/10 |STEP|  200/1719 |LOSS| 0.1478\n",
            "|EPOCHS| 9/10 |STEP|  400/1719 |LOSS| 0.1458\n",
            "|EPOCHS| 9/10 |STEP|  600/1719 |LOSS| 0.1474\n",
            "|EPOCHS| 9/10 |STEP|  800/1719 |LOSS| 0.1001\n",
            "|EPOCHS| 9/10 |STEP| 1000/1719 |LOSS| 0.1594\n",
            "|EPOCHS| 9/10 |STEP| 1200/1719 |LOSS| 0.2021\n",
            "|EPOCHS| 9/10 |STEP| 1400/1719 |LOSS| 0.2283\n",
            "|EPOCHS| 9/10 |STEP| 1600/1719 |LOSS| 0.1552\n",
            "--------------------------------------------------\n",
            "|* VALID SET *| |VAL LOSS| 0.2606 |ACC| 91.1500 |F1| 0.9114\n",
            "--------------------------------------------------\n",
            "|EPOCHS| 10/10 |STEP|  200/1719 |LOSS| 0.1755\n",
            "|EPOCHS| 10/10 |STEP|  400/1719 |LOSS| 0.1510\n",
            "|EPOCHS| 10/10 |STEP|  600/1719 |LOSS| 0.1995\n",
            "|EPOCHS| 10/10 |STEP|  800/1719 |LOSS| 0.1387\n",
            "|EPOCHS| 10/10 |STEP| 1000/1719 |LOSS| 0.0889\n",
            "|EPOCHS| 10/10 |STEP| 1200/1719 |LOSS| 0.0832\n",
            "|EPOCHS| 10/10 |STEP| 1400/1719 |LOSS| 0.1128\n",
            "|EPOCHS| 10/10 |STEP| 1600/1719 |LOSS| 0.1643\n",
            "--------------------------------------------------\n",
            "|* VALID SET *| |VAL LOSS| 0.2507 |ACC| 91.3600 |F1| 0.9133\n",
            "--------------------------------------------------\n",
            "Saving best model... F1 score is 0.9133\n",
            "Model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = RCNN(vocab_size=8000,\n",
        "            embedding_dim=300,\n",
        "            hidden_size=512,\n",
        "            hidden_size_linear=512,\n",
        "            class_num=4,\n",
        "            dropout=0.0).to(device)\n",
        "PATH = './best.pth'\n",
        "test_model.load_state_dict(torch.load(PATH))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdswIgTCL5Ni",
        "outputId": "8900c159-dc5a-41ea-816e-96c91c25de42"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_file_path = '/content/drive/MyDrive/Colab Notebooks/data_text/test.csv'\n",
        "test_texts, test_labels = read_file(test_file_path)\n",
        "test_dataset = CustomTextDataset(test_texts, test_labels, word2idx)\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                            collate_fn=lambda x: collate_fn(x, 64),\n",
        "                            batch_size=64,\n",
        "                            shuffle=True)\n",
        "\n",
        "_, accuracy, precision, recall, f1, cm = evaluate(test_model, test_dataloader)\n",
        "print('-'*50)\n",
        "print(f'|* TEST SET *| |ACC| {accuracy:>.4f} |PRECISION| {precision:>.4f} |RECALL| {recall:>.4f} |F1| {f1:>.4f}')\n",
        "print('-'*50)\n",
        "print('---------------- CONFUSION MATRIX ----------------')\n",
        "for i in range(len(cm)):\n",
        "  print(cm[i])\n",
        "print('--------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cOWIM7qju6s",
        "outputId": "0eda3dec-6696-4878-afaf-8fd2850f53ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "|* TEST SET *| |ACC| 90.7368 |PRECISION| 0.9071 |RECALL| 0.9074 |F1| 0.9072\n",
            "--------------------------------------------------\n",
            "---------------- CONFUSION MATRIX ----------------\n",
            "[1722   57   64   57]\n",
            "[  21 1848   16   15]\n",
            "[  58   19 1654  169]\n",
            "[  66   26  136 1672]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}