{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning_convolutional_Networks_for_Content-weighted_Image_Compression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkXjN0qMASv4nv8IV9FEhJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramprabu95/AI_based_image_compression/blob/main/Learning_convolutional_Networks_for_Content_weighted_Image_Compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gukcSjrFoKCc"
      },
      "source": [
        "# Learning Convolutional Networks for Content Weighted Image Compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhEHOO4OoHx1"
      },
      "source": [
        "# **Introduction:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWVsbvgX15EJ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkJokkGt6b6V"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBGPqxFv6cLT"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "# Definition of convolution layer\n",
        "def conv(ni, nf, kernal_size=3, stride=1, padding=1, **kwargs):\n",
        "    _conv = nn.Conv2d(ni, nf, kernel_size=kernal_size,stride=stride,padding=padding, **kwargs)\n",
        "    return _conv\n",
        "\n",
        "# Definition of a residual block\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, ni, nh=128):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv(ni, nh)\n",
        "        self.conv2 = conv(nh, ni)\n",
        "    def forward(self, x):\n",
        "        return x  + self.conv2(F.relu(self.conv1(x)))\n",
        "\n",
        "# Definition of Relu block\n",
        "relu = nn.ReLU()\n",
        "\n",
        "# Definition of encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.return_imp_map = return_imp_map\n",
        "        #\n",
        "        self.step1 = nn.Sequential(conv(ni=3, nf=128, kernal_size=8, stride=4, padding=2), relu,\n",
        "                                   ResBlock(nh=128), relu,\n",
        "                                   conv(ni=128, nf=256, kernal_size=4, stride=2, padding=1), relu,\n",
        "                                   ResBlock(nh=256), relu,\n",
        "                                   ResBlock(nh=256), relu)\n",
        "    def forward(self,x):\n",
        "        step1 = self.step1(x)\n",
        "        return(step1)\n",
        "\n",
        "# Function that does the binarizing action\n",
        "class Binarizing_function(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, i):\n",
        "    return (i>0.5).float()\n",
        "  \n",
        "  @staticmethod\n",
        "  def backward(ctx,grad_output):\n",
        "    return grad_output\n",
        "\n",
        "def binary_value(x):\n",
        "  return Binarizing_function.apply(x)\n",
        "\n",
        "class Bin(nn.Module):\n",
        "  def __init__(self,func):\n",
        "    super().__init__()\n",
        "    self.func = func\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.func(x)\n",
        "\n",
        "# Definition of binarize \n",
        "class BinarizerOrImportanceMap(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__(self,return_imp_map=False)\n",
        "\n",
        "    # Layer that contains the binarizer\n",
        "    self.step2 = nn.Sequential(Encoder(),\n",
        "                               conv(ni=256,nf=64,kernal_size=3,stride=1,padding=1),\n",
        "                               nn.Sigmoid(),\n",
        "                               Bin(binary_value))\n",
        "    \n",
        "    # Layer that contains the importance map \n",
        "    self.importance_map = nn.Sequential(Encoder(),\n",
        "                                        conv(256,128),\n",
        "                                        relu,\n",
        "                                        conv(128,128),\n",
        "                                        relu,\n",
        "                                        conv(128,1),\n",
        "                                        nn.Sigmoid())\n",
        "    \n",
        "    def forward(self,x):\n",
        "      if self.return_imp_map:return self.importance_map(x)\n",
        "      else: return self.step2(x)\n",
        "\n",
        "# Quantizer of the importance map\n",
        "class Quantizing(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, i):\n",
        "    p = i.clone()\n",
        "    L = 16\n",
        "    for l in range(L):\n",
        "      p[(p>=1/L)*(p<(l+1)/L)] = l\n",
        "    return p\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    return grad_output\n",
        "  \n",
        "def quantize_values(x):\n",
        "  return Quantizing.apply(x)\n",
        "\n",
        "class Masking(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx,i):\n",
        "    N,_,H,W = i.shape\n",
        "    n = 64 # as per the paper\n",
        "    L = 16 # as per the paper\n",
        "    mask = torch.zeros(n,N*H*W).to(device)\n",
        "    q_important = i\n",
        "    q_important_flat = q_important.view(1, N*H*W)\n",
        "    for index in range(n):\n",
        "      mask[index,:] = torch.where(index< n/L)*q_important_flat,torch.Tensor([1]).to(device),torch.Tensor([0]).to(device)\n",
        "    mask = mask.view(n,N,H,W).permute((0,1,2,3))\n",
        "    return mask\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "     N,_,H,W = grad_output.shape\n",
        "     if grad_output.is_cuda: return torch.ones(N,1,H,W).cuda()\n",
        "     else: return torch.ones(N,1,H,W)\n",
        "\n",
        "# function to generate mask\n",
        "def generateMask(x):\n",
        "  return Masking.apply(x)\n",
        "  \n",
        "\n",
        "# A decoder is to be defined which is a reverse of the encoder generated\n",
        "\n",
        "class BitstoImage(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self,blocksize):\n",
        "    super().__init__()\n",
        "    self.blocksize = blocksize\n",
        "  \n",
        "  def forward(self,x):\n",
        "    N,C,H,W = x.size()\n",
        "    x = x.view(N, self.blocksize, self.blocksize,C // (self.blocksize ** 2), H, W)\n",
        "    x = x.permute(0,3,4,1,5,2).contiguous()\n",
        "    x = x.view(N, C // (self.blocksize ** 2), H*self.blocksize, W*self.blocksize)\n",
        "    return x\n",
        "\n",
        "# Decoder design for rebulding the encoded data\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    # the convolution layers of a decoder is the reverse of the encoder\n",
        "    self.decoder = nn.Sequential(conv(ni=64,nf=512,kernal_size=1,stride=1,padding=0),\n",
        "                                 relu,\n",
        "                                 ResBlock(512),\n",
        "                                 relu,\n",
        "                                 ResBlock(512),\n",
        "                                 relu,\n",
        "                                 BitstoImage(2),\n",
        "                                 conv(ni=128, nf=256),\n",
        "                                 relu,\n",
        "                                 ResBlock(256),\n",
        "                                 relu,\n",
        "                                 BitstoImage(4),\n",
        "                                 conv(ni=16,nf=32),\n",
        "                                 relu,\n",
        "                                 conv(ni=32,nf=3))\n",
        "    def forward(self,x):\n",
        "      return self.decoder(x)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoXncqk76cff"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jozlth_b6cv4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjXtqPF0azvR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}